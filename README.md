# yolo11_nms_example

## abst

ultralytics 8.3.76でNMS出力が追加されたので、これを試した。

![image](https://github.com/user-attachments/assets/37eb4b3a-16cc-4abc-bf63-6e4c624f5ad5)
この写真は、ぱくたそオリジナルのフリー素材です。
https://www.pakutaso.com/20241213345post-53141.html

* v8.3.76 - `ultralytics 8.3.76` fix `dynamic` batch inference with NMS export (#19249)
https://github.com/ultralytics/ultralytics/releases/tag/v8.3.76


# Yolo




このデータ構造は一般的なYOLOモデル（YOLOv8など）の物体検出出力形式を表しています：

1. **1次元目 (1)**: バッチサイズ。一度に処理される画像の数で、通常は1枚。

2. **2次元目 (300)**: 最大検出数。モデルが検出できる最大物体数を示します。実際に検出された物体数がこれより少ない場合、残りのエントリは低い信頼度値で埋められます。

3. **3次元目 (6)**: 各検出の詳細情報で、以下の要素が含まれます：
   - インデックス 0-3: バウンディングボックス座標
     - format='xyxy'の場合: [x1, y1, x2, y2] (左上と右下の座標)
     - format='xywh'の場合: [x_center, y_center, width, height] (中心座標と幅・高さ)
   - インデックス 4: 検出信頼度スコア（0〜1の範囲）
   - インデックス 5: クラスID（整数値、検出された物体のカテゴリを示す）

コード内の処理を見ると、このデータを次のように扱っています：

```python
for detection in detections:
    if len(detection) >= 6:  # 座標とスコアとクラス
        score = detection[4]
        if score > conf_threshold:
            valid_detections.append(detection)
```

この6要素の構造は、その後、以下のように処理されます：
- 最初の4つの値がバウンディングボックス座標として取り出される
- 5番目の値が信頼度スコアとして使用される
- 6番目の値が物体のクラスIDとして解釈される

YOLOPose (1,300,57) の形式と比較すると、この (1,300,6) 形式はキーポイント情報を含まない基本的な物体検出出力です。これは通常のYOLOモデルの出力形式で、単純に物体の位置とクラスのみを検出します。




# Yolo-Pose

### データ構造の詳細

**形状 (1, 300, 57)** の意味:

1. **1**: バッチサイズ。一度に処理される画像の数で、通常は1枚。

2. **300**: 最大検出数。YOLOv8-poseモデルが検出できる最大物体（人物）数。実際に検出された人物数がこれより少ない場合、残りのエントリは低い信頼度値で埋められます。

3. **57**: 各検出の詳細情報で、以下のように構成されています:

   - インデックス 0-3: バウンディングボックス座標 (x1, y1, x2, y2)
   - インデックス 4: 検出信頼度スコア
   - インデックス 5: クラスID（YOLOv8-poseでは通常0、人物を表す）
   - インデックス 6-56: 17個のキーポイント情報（各キーポイントは x, y, 信頼度 の3つの値を持つ）

### キーポイントの構成

コードから、キーポイントの部分は以下のように解釈できます:

```python
# キーポイントの処理（インデックス6から始まる）
for kpt_idx in range(17):
    # キーポイントのインデックス計算（各キーポイントはx, y, confの3つの値）
    base_idx = 6 + (kpt_idx * 3)
    kx, ky, kp_conf = detection[base_idx:base_idx+3]
```

17個のキーポイントの順序は以下の通りです（COCOデータセット形式）:

```python
KEYPOINT_NAMES = [
    "nose", "left_eye", "right_eye", "left_ear", "right_ear",
    "left_shoulder", "right_shoulder", "left_elbow", "right_elbow", 
    "left_wrist", "right_wrist", "left_hip", "right_hip", 
    "left_knee", "right_knee", "left_ankle", "right_ankle"
]
```

各キーポイントの位置:
- 鼻: インデックス 6, 7, 8 (x, y, 信頼度)
- 左目: インデックス 9, 10, 11
- 右目: インデックス 12, 13, 14
- その他のキーポイントも同様の3つ組で続く

### 検出処理のフロー

コードから見ると、検出処理は以下の手順で行われています:

1. スコア（インデックス4）が閾値以上の検出を抽出
2. 各検出のバウンディングボックス座標を元の画像サイズにスケーリング
3. 各キーポイントの座標と信頼度を抽出し、元の画像サイズにスケーリング
4. 信頼度が閾値以上のキーポイントと骨格線を描画

### 特記事項

- インデックス5はクラスIDとして使用されています（人体検出では通常0）
- キーポイントの信頼度値は正規化されていないため、表示前に閾値と比較するだけで使用されています
- 他のコードからはキーポイントの信頼度値が通常より高い値（200-550範囲）であることが示唆されていますが、このコードではその点についての特別な処理は行われていません

この構造はYOLOv8-poseに特有のもので、プロジェクトによって若干異なる場合があります。

